# Brief: GPU Scaling & Qwen Integration

## Goal
Обеспечить масштабируемость инференса на GPU (поддержка CUDA для A16 и RTX 3060Ti) и внедрить постобработку текста с помощью LLM Qwen для улучшения качества транскрибации.

## Context
Текущая реализация сервера использует `whisper-rs`. Для продакшн-нагрузки (Nvidia A16) и локального тестирования (RTX 3060Ti) необходимо задействовать GPU. Также требуется добавить этап постобработки текста для исправления ошибок распознавания и форматирования с помощью LLM (Qwen).

## Requirements

### 1. GPU Acceleration
- Включить поддержку CUDA для `whisper-rs`.
- Реализовать механизм выбора устройства (CPU/GPU) через конфигурацию или аргументы командной строки.
- Обеспечить совместимость с Nvidia A16 (Ampere) и RTX 3060Ti (Ampere).

### 2. Qwen Integration
- Интегрировать LLM `Qwen2-7B-Instruct` (или аналогичную, например, `Phi-3`) для постобработки.
- Использовать квантованные модели (GGUF/GGML) для экономии VRAM (целевой бюджет < 8GB для 3060Ti).
- Реализовать пайплайн: `Audio Stream` -> `Whisper ASR` -> `Raw Text` -> `Qwen Post-processing` -> `Clean Text`.

### 3. Configuration
- Добавить настройки для путей к моделям (Whisper, LLM).
- Добавить настройки параметров генерации (temperature, tokens).

## Technical Constraints
- **VRAM**: RTX 3060Ti имеет 8GB VRAM. Whisper Large v3 (~3GB) + Qwen2-7B-Q4 (~5GB) = ~8GB. Необходимо следить за потреблением памяти или использовать меньшие модели (Medium/Small или Qwen-1.5-4B/Phi-3) при нехватке памяти.
- **Latency**: Постобработка не должна вносить критическую задержку в real-time поток (возможно, обрабатывать чанками или предложениями).

## Acceptance Criteria
- [ ] Сервер запускается с флагом использования GPU.
- [ ] `whisper-rs` исполняется на GPU (проверка через `nvidia-smi`).
- [ ] Текст после Whisper передается в LLM.
- [ ] LLM возвращает откорректированный текст.
- [ ] Система работает стабильно на RTX 3060Ti (не падает по OOM).